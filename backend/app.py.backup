import io
import os
from flask import Flask, request, jsonify
from flask_cors import CORS
import PyPDF2
from dotenv import load_dotenv
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from transformers import AutoTokenizer, AutoModel
import torch
import re

# Load environment variables from .env
load_dotenv()

# Initialize BERT model and tokenizer
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
model = AutoModel.from_pretrained('bert-base-uncased')

# Flask app initialization
app = Flask(__name__)
CORS(app, origins=[
    "http://localhost:5173",
    "https://me-profiled-frontend.vercel.app"
])

# PDF text extraction
def extract_text_from_pdf(pdf_file):
    try:
        pdf_reader = PyPDF2.PdfReader(pdf_file)
        text = ""
        for page in pdf_reader.pages:
            page_text = page.extract_text()
            if page_text:
                text += page_text
        return text
    except Exception as e:
        print(f"Error reading PDF: {e}")
        return ""

# Get BERT embeddings for text
def get_bert_embeddings(text):
    # Tokenize and encode
    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512, padding=True)
    
    # Get model output
    with torch.no_grad():
        outputs = model(**inputs)
    
    # Use mean pooling on token embeddings
    embeddings = outputs.last_hidden_state.mean(dim=1)
    return embeddings.numpy()

# Extract keywords from text
def extract_keywords(text):
    # Simple keyword extraction - lowercase and split
    text = text.lower()
    # Remove special characters
    text = re.sub(r'[^a-zA-Z0-9\s+#]', ' ', text)
    words = set(text.split())
    # Filter out common words (basic stop words)
    stop_words = {'a', 'an', 'the', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 
                  'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'should',
                  'can', 'could', 'may', 'might', 'must', 'and', 'or', 'but', 'in', 
                  'on', 'at', 'to', 'for', 'of', 'with', 'by', 'from', 'as', 'that'}
    return words - stop_words

# Calculate match scores
def calculate_match_score(resume_text, job_description):
    # Get embeddings
    resume_emb = get_bert_embeddings(resume_text)
    job_emb = get_bert_embeddings(job_description)
    
    # Calculate semantic similarity (0 to 1)
    semantic_similarity = cosine_similarity(resume_emb, job_emb)[0][0]
    
    # Extract keywords
    resume_keywords = extract_keywords(resume_text)
    job_keywords = extract_keywords(job_description)
    
    # Calculate keyword match
    common_keywords = resume_keywords.intersection(job_keywords)
    keyword_match = len(common_keywords) / len(job_keywords) if len(job_keywords) > 0 else 0
    
    # Calculate scores (using semantic similarity as base)
    # Skills match: 70% semantic + 30% keyword
    skills_match = int((semantic_similarity * 0.7 + keyword_match * 0.3) * 100)
    
    # Experience match: based on semantic similarity
    experience_match = int(semantic_similarity * 100)
    
    # Keyword match percentage
    keyword_match_percent = int(keyword_match * 100)
    
    # Final score calculation
    match_score = int((skills_match * 0.50) + (experience_match * 0.40) + (keyword_match_percent * 0.10))
    
    return match_score, skills_match, experience_match, keyword_match_percent, common_keywords

# Generate analysis based on scores
def generate_analysis(match_score, skills_match, experience_match, keyword_match, common_keywords, resume_text, job_description):
    # Generate summary
    if match_score >= 85:
        summary = f"Strong match with {match_score}% overall compatibility. The candidate's profile aligns well with the job requirements and demonstrates relevant expertise."
    elif match_score >= 70:
        summary = f"Moderate match with {match_score}% overall compatibility. The candidate shows good potential with some areas that could be enhanced to better align with the role."
    else:
        summary = f"Developing match with {match_score}% overall compatibility. The candidate has foundational qualities but would benefit from developing additional skills and experience for this role."
    
    # Generate strengths
    strengths = []
    if skills_match >= 75:
        strengths.append("Strong technical skills alignment with job requirements")
    if experience_match >= 75:
        strengths.append("Relevant work experience matching the role's expectations")
    if keyword_match >= 60:
        strengths.append("Good coverage of key technologies and tools mentioned in job description")
    if len(common_keywords) > 10:
        strengths.append(f"Demonstrates knowledge of {len(common_keywords)} relevant keywords and technologies")
    if not strengths:
        strengths.append("Shows foundational knowledge in the field")
    
    # Generate areas for improvement
    improvements = []
    if skills_match < 75:
        improvements.append("Enhance technical skills section to better match job requirements")
    if experience_match < 75:
        improvements.append("Highlight more relevant work experience and projects related to the role")
    if keyword_match < 60:
        improvements.append("Include more specific technologies, tools, and frameworks mentioned in the job description")
    if len(common_keywords) < 10:
        improvements.append("Add industry-specific keywords and technical terminology from the job posting")
    improvements.append("Tailor resume content to emphasize achievements that directly relate to job responsibilities")
    
    return {
        "matchScore": match_score,
        "skillsMatchPercent": skills_match,
        "experienceMatchPercent": experience_match,
        "keywordMatchPercent": keyword_match,
        "summary": summary,
        "strengths": strengths[:5],  # Limit to 5
        "areasForImprovement": improvements[:5]  # Limit to 5
    }

# AI resume analyzer endpoint
@app.route('/', methods=['GET'])
def home():
    return jsonify({"message": "Hello from Backend"}), 200

@app.route('/analyze', methods=['POST'])
def analyze_resume():
    if 'resume' not in request.files:
        return jsonify({'error': 'No resume file provided'}), 400

    job_description = request.form.get('jobDescription', '')
    if not job_description:
        return jsonify({'error': 'No job description provided'}), 400

    resume_file = request.files['resume']
    resume_text = extract_text_from_pdf(io.BytesIO(resume_file.read()))

    if not resume_text:
        return jsonify({'error': 'Could not extract text from the resume PDF. The file might be empty, corrupted, or image-based.'}), 500

    try:
        # Calculate match scores using BERT
        match_score, skills_match, experience_match, keyword_match, common_keywords = calculate_match_score(
            resume_text, job_description
        )
        
        # Generate detailed analysis
        analysis_result = generate_analysis(
            match_score, skills_match, experience_match, keyword_match, 
            common_keywords, resume_text, job_description
        )

        return jsonify(analysis_result)

    except Exception as e:
        print(f"An error occurred during analysis: {e}")
        return jsonify({'error': f'Failed to analyze resume: {str(e)}'}), 500

        # Prompt with scoring logic
        prompt_template = """
You are an expert AI career coach and resume analyzer.
Your goal is to provide a detailed, constructive analysis of a user's resume against a specific job description.

**Resume Content:**
{resume}

**Job Description:**
{job_description}

**Analysis Instructions:**

1. **Match Score Calculation:**  
   Calculate a final match score from 0 to 100 using the following weight distribution:
   - **Skills Match (50%)**: Check how many of the technical and soft skills from the job description are mentioned or implied in the resume.
   - **Experience Match (40%)**: Evaluate how well the candidate’s past roles, industries, and years of experience align with the job requirements.
   - **Keyword Match (10%)**: Identify important keywords from the job description (e.g., tools, technologies, frameworks, methodologies) and how many of them appear in the resume.

   Calculate individual scores for each of the three categories:
   matchScore = (skillsMatchPercent * 0.50) + (experienceMatchPercent * 0.40) + (keywordMatchPercent * 0.10)

   Round all percentage values and the final score to the nearest integer. A score of 85 or above indicates a strong match.

2. **Analysis Summary:**  
   Write a 2–3 sentence overview. Clearly state whether the candidate is a strong, moderate, or weak fit and summarize key alignment areas.

3. **Strengths:**  
   List 3–5 things the candidate does well that are directly relevant to the job (skills, technologies, experience, etc.).

4. **Areas for Improvement:**  
   List 3–5 areas where the resume could be improved to better match the job. Focus on **missing skills**, **important keywords**, or **experience gaps**. Be actionable and specific.

**Output Format:**  
Return the response strictly in the following JSON format. Do not include any other explanation or markdown formatting.

{{
    "matchScore": <number>,
    "skillsMatchPercent": <number>,
    "experienceMatchPercent": <number>,
    "keywordMatchPercent": <number>,
    "summary": "<string>",
    "strengths": ["<string>", "<string>", ...],
    "areasForImprovement": ["<string>", "<string>", ...]
}}
"""

        prompt = ChatPromptTemplate.from_template(prompt_template)
        parser = JsonOutputParser()
        chain = prompt | llm | parser

        # Invoke AI agent
        analysis_result = chain.invoke({
            "resume": resume_text,
            "job_description": job_description
        })

        return jsonify(analysis_result)

    except Exception as e:
        print(f"An error occurred during AI analysis: {e}")
        return jsonify({'error': 'Failed to get analysis from AI agent.'}), 500

# Start server
if __name__ == '__main__':
    if not os.getenv("OPENAI_API_KEY"):
        raise ValueError("OPENAI_API_KEY not found in .env file. Please add it.")
    app.run(debug=True, port=5001)
